{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NVAA8b9i9NHH",
        "outputId": "3226e78e-12e1-4c23-9edc-f944a86e5d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2,>=1.2 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, google-ai-generativelanguage, langchain-google-genai, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.7.0 langchain-community-0.3.31 langchain-google-genai-2.1.12 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ca4346008bef4801bab580f787ba922a",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install langchain langchain-community langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt7z7srL9cQ5"
      },
      "source": [
        "# Chat Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQEpzBn59hum"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = '<sua chave aqui>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-gYn8tby9i4M"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ras5ussM9z95"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zNOfO6l956Y",
        "outputId": "0bbac79a-4450-4f72-e3c0-5b02d59c1577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Oi! Sou eu, o Gemini. Tudo bem por aí? Como posso ajudar? 😊', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5413a586-9605-431a-91fa-0a274afab9b8-0', usage_metadata={'input_tokens': 6, 'output_tokens': 295, 'total_tokens': 301, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 277}})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke([{\n",
        "     \"role\": \"user\",\n",
        "     \"content\": \"Oi, gemini!\"\n",
        "     }])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IlyRzc_F-B5S"
      },
      "outputs": [],
      "source": [
        "resposta = llm.invoke([{\n",
        "     \"role\": \"user\",\n",
        "     \"content\": \"Oi, gemini!\"\n",
        "     }])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0wsPsDvx-GBr",
        "outputId": "73a7bbbe-d69f-41ab-d0e7-75526987eeb9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Oi! Sou o Gemini. Como posso ajudar hoje?'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resposta.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1KubmHH-oTB",
        "outputId": "159ae7ed-64bb-4e91-85dd-7c914036afc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Uma **LLM** significa **Large Language Model** (Modelo de Linguagem Grande).\\n\\nÉ um tipo de modelo de inteligência artificial (IA), especificamente dentro da área de aprendizado de máquina (Machine Learning), projetado para **ent' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_tokens': 8, 'output_tokens': 1980, 'total_tokens': 1988, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1930}}content='ender, gerar e interagir com a linguagem humana** de forma muito sofisticada.\\n\\nVamos quebrar o significado de cada parte:\\n\\n1.  **Grande (Large):**\\n    *   Refere-se ao tamanho massivo' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 49, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49}content=' dos dados de treinamento e ao número gigantesco de \"parâmetros\" (as variáveis internas que o modelo ajusta durante o treinamento). Estamos falando de trilhões de palavras e bilhões ou até trilhões de parâmetros. É essa escala que' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content=' permite ao modelo capturar nuances complexas da linguagem.\\n\\n2.  **Linguagem (Language):**\\n    *   O foco principal é a linguagem natural humana (texto). O modelo é treinado para processar e gerar' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 49, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49}content=' texto de forma coerente e contextualmente relevante, imitando a forma como os humanos se comunicam.\\n\\n3.  **Modelo (Model):**\\n    *   É um programa de computador complexo, geralmente baseado em arquiteturas' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content=' de redes neurais profundas (especialmente a arquitetura \"Transformer\"), que foi treinado para aprender padrões e relações na linguagem.\\n\\n**Como funcionam (de forma simplificada):**\\n\\nAs LLMs são treinadas em quantidades *en' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 50, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50}content='ormes* de dados textuais da internet (livros, artigos, conversas, etc.). Durante esse treinamento, o modelo aprende a prever a próxima palavra em uma sequência, a entender o contexto, a gramática, a sem' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content='ântica e até mesmo o estilo.\\n\\nO que os torna tão poderosos é a sua capacidade de **generalizar** esse conhecimento. Eles não apenas memorizam frases, mas aprendem as *regras* e *padrões' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content='* subjacentes da linguagem, permitindo-lhes realizar uma vasta gama de tarefas.\\n\\n**O que uma LLM pode fazer?**\\n\\nGraças ao seu treinamento massivo, uma LLM pode:\\n*   Gerar' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 49, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49}content=' texto (artigos, e-mails, roteiros, poesia).\\n*   Responder a perguntas de forma informativa.\\n*   Traduzir idiomas.\\n*   Resumir documentos longos.\\n*   Escrever código de programação' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 50, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50}content='.\\n*   Criar chatbots e assistentes virtuais.\\n*   Analisar sentimentos em textos.\\n*   E muito mais, dependendo da instrução (prompt) que você dá a ela.\\n\\n**Características Ch' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 49, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49}content='ave:**\\n\\n*   **Compreensão Contextual:** Conseguem entender o significado e as nuances de uma frase ou parágrafo.\\n*   **Geração Coerente:** Produzem texto que é gramaticalmente correto' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content=' e faz sentido no contexto.\\n*   **Versatilidade:** Podem ser usados para uma infinidade de tarefas relacionadas à linguagem.\\n*   **Aprendizado em Contexto (In-context learning):** Com apenas algumas instruções' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content=' ou exemplos (prompts), podem adaptar-se a novas tarefas sem a necessidade de retreinamento.\\n\\n**Exemplos notáveis incluem:** ChatGPT (da OpenAI), Gemini (do Google), Llama (da Meta), Claude (da' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 49, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49}content=' Anthropic), entre outros.\\n\\nPense em uma LLM como um **bibliotecário superinteligente** que leu praticamente todo o conteúdo textual disponível na internet e em livros. Ele não apenas memorizou tudo, mas também entendeu' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content=' como as palavras se conectam, como as ideias são formadas e como a linguagem é usada em diferentes contextos. Por isso, ele consegue responder a perguntas, escrever novos textos e até mesmo ter \"conversas\" coerentes.\\n\\nEm' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 48, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48}content=' resumo, as LLMs são uma das tecnologias mais revolucionárias da IA moderna, com um potencial imenso para transformar a forma como interagimos com a informação e com as máquinas.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--a4f70ee6-6e71-408e-8571-fb3b55faf355' usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 37, 'input_tokens': 0, 'output_token_details': {'reasoning': 0}, 'total_tokens': 37}"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "async def stream_response():\n",
        "    async for chunk in llm.astream(\"O que é uma LLM?\"):\n",
        "        print(chunk, end=\"\")\n",
        "\n",
        "await stream_response()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF1NPqFr-cJb"
      },
      "source": [
        "# Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "U0dRRtbn-dvU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AKy6CGcq_aOt"
      },
      "outputs": [],
      "source": [
        "prompt_template = PromptTemplate.from_template(\"Me conte uma piada sobre {topic}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt_Fcom2_dmG",
        "outputId": "50b1c0fe-a77c-4afc-83ad-3ca786b28faa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StringPromptValue(text='Me conte uma piada sobre gatos')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template.invoke({\"topic\": \"gatos\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ5oxjDP_gKL",
        "outputId": "a38ac547-77c7-4129-dea6-c208754e1a9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Você é um assistente prestativo', additional_kwargs={}, response_metadata={}), HumanMessage(content='Me conte uma piada sobre gatos', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"Você é um assistente prestativo\"),\n",
        "    (\"user\", \"Me conte uma piada sobre {topic}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"gatos\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nqLSseGJ_qYb"
      },
      "outputs": [],
      "source": [
        "contador_de_piadas = prompt_template | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjI19ehw_0YI",
        "outputId": "233285b6-6cbe-4b8d-ec5d-b501a6255039"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Claro! Aqui vai uma:\\n\\n**Por que os elefantes não usam computador?**\\n\\n... Porque eles têm medo do mouse! 😂', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e0fe14b2-ac49-4a87-acfe-8b1670a91c9f-0', usage_metadata={'input_tokens': 17, 'output_tokens': 148, 'total_tokens': 165, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 118}})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contador_de_piadas.invoke({\"topic\": \"elefantes\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSI8NQOJAA_M"
      },
      "source": [
        "# Output Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YY--qA5D_6Gr"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BInEAhOuAZ6_"
      },
      "outputs": [],
      "source": [
        "chain1 = contador_de_piadas | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MFKjRimjAdPS",
        "outputId": "bea2c968-9a60-4d2b-a0f7-8d6d529cd562"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Claro! Aqui vai uma:\\n\\nPor que o fantasma foi ao bar?\\n.\\n.\\n.\\nPara pegar uma **bebida espirituosa**! 👻🍹'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain1.invoke({\"topic\": \"fantasmas\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWgUUsniArUX"
      },
      "source": [
        "Também pode ser usado pra outros formatos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "q8B_oDY4Apq3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6e8BT74iA-vp"
      },
      "outputs": [],
      "source": [
        "class Piada(BaseModel):\n",
        "    pergunta: str = Field(description=\"esse campo representa a pergunta para gerar a piada\")\n",
        "    resposta: str = Field(description=\"esse campo representa a resposta da piada\")\n",
        "\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Piada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtSlgqiTBXJ_",
        "outputId": "fc118587-75da-40a0-8e91-3dc6e5f3709c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'pergunta': 'Por que o computador foi ao oculista?',\n",
              " 'resposta': 'Porque ele estava com a tela embaçada!'}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"Crie uma piada sobre o tópico.\n",
        "    Sua resposta deve ser um json no seguinte formato: {format_instructions}\n",
        "    Tópico: {topic}\n",
        "    \"\"\",\n",
        "    input_variables=[\"topic\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0)\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "chain.invoke({\"topic\": \"computadores\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwTIV00mCZ4T"
      },
      "source": [
        "# Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl4-vMLPD--i"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8_XFWNTgC2OF"
      },
      "outputs": [],
      "source": [
        "prompt_critico_de_piadas =  PromptTemplate.from_template(\"Piada: {piada}. Responda no formato: Piada: <piada>\\n Nota <nota de 0 a 9>\\n Justificativa: <explique o porque da nota>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kiREvjqgDbnX"
      },
      "outputs": [],
      "source": [
        "julgador = contador_de_piadas | prompt_critico_de_piadas | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "WpNdUx6KEkGA",
        "outputId": "e5159bb2-cc34-46e3-b780-bfcbd6bdc318"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.runnables.base.RunnableSequence</b><br/>def __init__(*steps: RunnableLike, name: Optional[str]=None, first: Optional[Runnable[Any, Any]]=None, middle: Optional[list[Runnable[Any, Any]]]=None, last: Optional[Runnable[Any, Any]]=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py</a>Sequence of ``Runnables``, where the output of each is the input of the next.\n",
              "\n",
              "**``RunnableSequence``** is the most important composition operator in LangChain\n",
              "as it is used in virtually every chain.\n",
              "\n",
              "A ``RunnableSequence`` can be instantiated directly or more commonly by using the\n",
              "``|`` operator where either the left or right operands (or both) must be a\n",
              "``Runnable``.\n",
              "\n",
              "Any ``RunnableSequence`` automatically supports sync, async, batch.\n",
              "\n",
              "The default implementations of ``batch`` and ``abatch`` utilize threadpools and\n",
              "asyncio gather and will be faster than naive invocation of ``invoke`` or ``ainvoke``\n",
              "for IO bound ``Runnable``s.\n",
              "\n",
              "Batching is implemented by invoking the batch method on each component of the\n",
              "``RunnableSequence`` in order.\n",
              "\n",
              "A ``RunnableSequence`` preserves the streaming properties of its components, so if\n",
              "all components of the sequence implement a ``transform`` method -- which\n",
              "is the method that implements the logic to map a streaming input to a streaming\n",
              "output -- then the sequence will be able to stream input to output!\n",
              "\n",
              "If any component of the sequence does not implement transform then the\n",
              "streaming will only begin after this component is run. If there are\n",
              "multiple blocking components, streaming begins after the last one.\n",
              "\n",
              ".. note::\n",
              "    ``RunnableLambdas`` do not support ``transform`` by default! So if you need to\n",
              "    use a ``RunnableLambdas`` be careful about where you place them in a\n",
              "    ``RunnableSequence`` (if you need to use the ``stream``/``astream`` methods).\n",
              "\n",
              "    If you need arbitrary logic and need streaming, you can subclass\n",
              "    Runnable, and implement ``transform`` for whatever logic you need.\n",
              "\n",
              "Here is a simple example that uses simple functions to illustrate the use of\n",
              "``RunnableSequence``:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.runnables import RunnableLambda\n",
              "\n",
              "\n",
              "        def add_one(x: int) -&gt; int:\n",
              "            return x + 1\n",
              "\n",
              "\n",
              "        def mul_two(x: int) -&gt; int:\n",
              "            return x * 2\n",
              "\n",
              "\n",
              "        runnable_1 = RunnableLambda(add_one)\n",
              "        runnable_2 = RunnableLambda(mul_two)\n",
              "        sequence = runnable_1 | runnable_2\n",
              "        # Or equivalently:\n",
              "        # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
              "        sequence.invoke(1)\n",
              "        await sequence.ainvoke(1)\n",
              "\n",
              "        sequence.batch([1, 2, 3])\n",
              "        await sequence.abatch([1, 2, 3])\n",
              "\n",
              "Here&#x27;s an example that uses streams JSON output generated by an LLM:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
              "        from langchain_openai import ChatOpenAI\n",
              "\n",
              "        prompt = PromptTemplate.from_template(\n",
              "            &quot;In JSON format, give me a list of {topic} and their &quot;\n",
              "            &quot;corresponding names in French, Spanish and in a &quot;\n",
              "            &quot;Cat Language.&quot;\n",
              "        )\n",
              "\n",
              "        model = ChatOpenAI()\n",
              "        chain = prompt | model | SimpleJsonOutputParser()\n",
              "\n",
              "        async for chunk in chain.astream({&quot;topic&quot;: &quot;colors&quot;}):\n",
              "            print(&quot;-&quot;)  # noqa: T201\n",
              "            print(chunk, sep=&quot;&quot;, flush=True)  # noqa: T201</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2857);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "langchain_core.runnables.base.RunnableSequence"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(julgador)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8tpgu6mD2AY",
        "outputId": "bea5e9b2-878f-4c0c-ed08-8fd5e63f6a46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Piada: Aqui vai uma:\\n\\nPor que os gatos são tão ruins em jogos de cartas?\\nPorque eles sempre têm uma **pata** de azar! 😼\\nNota 7\\nJustificativa: A piada é um trocadilho inteligente e bem construído em português. A expressão \"pata de azar\" é uma brincadeira com \"má sorte\" e a palavra \"pata\" se encaixa perfeitamente no contexto dos gatos. É uma piada leve, divertida e que arranca um sorriso, mesmo que seja um \"riso de trocadilho\". A execução é clara e o emoji complementa bem.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5e8a1896-3101-45d7-9cba-77fdf03c76ea-0', usage_metadata={'input_tokens': 236, 'output_tokens': 729, 'total_tokens': 965, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 596}})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "julgador.invoke({\"topic\": \"gatos\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
